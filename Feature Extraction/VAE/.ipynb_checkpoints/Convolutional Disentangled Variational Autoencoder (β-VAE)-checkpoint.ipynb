{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7605"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# PyTorch 4.1+\n",
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.utils as vutils\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "%matplotlib inline\n",
    "torch.backends.cudnn.version()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional β-Variational Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = True\n",
    "\n",
    "ZDIMS = 20\n",
    "BETA = 5\n",
    "LR = 1e-3\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 64\n",
    "SEED = 4\n",
    "LOG_INTERVAL = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "device = torch.device('cpu')\n",
    "# kwargs = {'num_workers': 1, 'pin_memory': True} if CUDA else {}\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build train data\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train = True,\n",
    "                   download = True,\n",
    "                   transform = transforms.ToTensor()),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# build test data\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data',\n",
    "                   train = False,\n",
    "                   transform = transforms.ToTensor()),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    **kwargs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for batch, (data, _) in enumerate(train_loader):\n",
    "    if batch == 2:\n",
    "        print(data.shape)\n",
    "        plt.imshow(data[63,0,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "Because I plan on using this for dimension reduction, I based the architecture loosely on Convolutional Variational Autoencoder featured in the influential World Models research paper. It has been modified with a β coefficient for disentangling the latent features/vectors.\n",
    " - https://worldmodels.github.io/\n",
    " - https://github.com/hardmaru/WorldModelsExperiments\n",
    " - https://github.com/hardmaru/WorldModelsExperiments/blob/master/carracing/vae/vae.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NC = 1    # channels\n",
    "NEF = 64  # init encoding filters\n",
    "NDF = 64  # init decoding filters\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, zdims):\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        self.zdims = zdims\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            \n",
    "            # input is (NC) x 28 x 28 (MNIST)\n",
    "            nn.Conv2d(in_channels = NC, out_channels = NEF, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 2\n",
    "            nn.Conv2d(in_channels = NEF, out_channels = NEF * 2, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NEF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 3\n",
    "            nn.Conv2d(in_channels = NEF * 2, out_channels = NEF * 4, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NEF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # conv layer 4\n",
    "            nn.Conv2d(in_channels = NEF * 4, out_channels = 1024, kernel_size = 4, stride = 2, padding = 1),\n",
    "            #nn.BatchNorm2d(1024), # OPTIONAL\n",
    "            nn.ReLU(inplace = True)\n",
    "\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "\n",
    "            # input is Z (post-fc)\n",
    "            nn.ConvTranspose2d(in_channels = 1024, out_channels = NDF * 8, kernel_size = 4, stride = 1, padding = 0),\n",
    "            nn.BatchNorm2d(NDF * 8),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 2\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 8, out_channels = NDF * 4, kernel_size = 3, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NDF * 4),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 3\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 4, out_channels = NDF * 2, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.BatchNorm2d(NDF * 2),\n",
    "            nn.ReLU(inplace = True),\n",
    "\n",
    "            # deconv layer 4\n",
    "            nn.ConvTranspose2d(in_channels = NDF * 2, out_channels = NC, kernel_size = 4, stride = 2, padding = 1),\n",
    "            nn.Sigmoid()\n",
    "\n",
    "        )\n",
    "        \n",
    "        # conv fc\n",
    "        self.fc11 = nn.Linear(1024, self.zdims) # mu\n",
    "        self.fc12 = nn.Linear(1024, self.zdims) # logvar\n",
    "        \n",
    "        # deconv fc\n",
    "        self.fc2  = nn.Linear(self.zdims, 1024)\n",
    "    \n",
    "    \n",
    "    def encode(self, x):\n",
    "        conv = self.encoder(x)\n",
    "        conv = conv.view(-1, 1024)\n",
    "        mu = self.fc11(conv)\n",
    "        logvar = self.fc12(conv)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        deconv_input = F.relu(self.fc2(z))\n",
    "        deconv_input = deconv_input.view(-1, 1024, 1, 1) # world models: [-1, 1, 1, 1024]\n",
    "        recon_x = self.decoder(deconv_input)\n",
    "        return recon_x\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(logvar * 0.5)\n",
    "        eps = torch.rand_like(std)\n",
    "        z = eps.mul(std).add(mu)\n",
    "        return z\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        recon_x = self.decode(z)\n",
    "        return recon_x, mu, logvar, z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VAE(zdims = ZDIMS).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr = LR)\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, logvar, beta = 1):\n",
    "    '''Use a beta value of 1 for a vanilla VAE'''\n",
    "    \n",
    "    # loss\n",
    "    BCE = F.binary_cross_entropy(recon_x, x.view(64, 28, 28), reduction = 'sum')\n",
    "    \n",
    "    # KL Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    \n",
    "    return BCE + (beta * KLD)\n",
    "\n",
    "def train(epoch, beta = 1):\n",
    "    \n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    \n",
    "    for batch_idx, (data, _) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar, Z = model(data)\n",
    "        loss = loss_function(recon_batch, data, mu, logvar, beta)\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % LOG_INTERVAL == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch,\n",
    "                batch_idx * len(data),\n",
    "                len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader),\n",
    "                loss.item() / len(data)\n",
    "            ))\n",
    "    \n",
    "    print('====> Epoch: {} Average loss: {:.4f}'.format(\n",
    "        epoch,\n",
    "        train_loss / len(train_loader.dataset)\n",
    "    ))\n",
    "    \n",
    "def test(epoch):\n",
    "    \n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i, (data, _) in enumerate(test_loader):\n",
    "            data = data.to(device)\n",
    "            recon_batch, mu, logvar, Z = model(data)\n",
    "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
    "            \n",
    "            if i == 0:\n",
    "                n = min(data.size(0), 8)\n",
    "                comparison = torch.cat([data[:n], recon_batch.view(BATCH_SIZE, 1, 28, 28)[:n]])\n",
    "                save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow = n)\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('====> Test set loss: {:.4f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nick/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: UserWarning: Using a target size (torch.Size([64, 784])) that is different to the input size (torch.Size([64, 1, 28, 28])) is deprecated. Please ensure they have the same size.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (28) must match the size of tensor b (784) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5d9fb973782d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBETA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# note: beta not currently considered in validation error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-33f7563b0477>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, beta)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogvar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-59-33f7563b0477>\u001b[0m in \u001b[0;36mloss_function\u001b[0;34m(recon_x, x, mu, logvar, beta)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mBCE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecon_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m784\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'sum'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# KL Divergence = 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy\u001b[0;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2378\u001b[0m     return torch._C._nn.binary_cross_entropy(\n\u001b[0;32m-> 2379\u001b[0;31m         input, target, weight, reduction_enum)\n\u001b[0m\u001b[1;32m   2380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (28) must match the size of tensor b (784) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(epoch, beta = BETA) # note: beta not currently considered in validation error\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'results/sample_1.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-119ad6656796>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mZDIMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'results/sample_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, fp, nrow, padding, normalize, range, scale_each, pad_value, format)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m     \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2097\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2098\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2101\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'results/sample_1.png'"
     ]
    }
   ],
   "source": [
    "# sample from latent space\n",
    "with torch.no_grad():\n",
    "    sample = torch.randn(64, ZDIMS).to(device)\n",
    "    sample = model.decode(sample).cpu()\n",
    "    save_image(sample.view(64, 1, 28, 28), 'results/sample_' + str(epoch) + '.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Results\n",
    "\n",
    "*NOTE: because the input data (MNIST vs World Models) is very different, most of the layer hyperparams had to be adjusted. Otherwise, the biggest difference is that Batch Normalization, which showed a lower training and validation error, is used.\n",
    "\n",
    " - **Modified World Models**\n",
    "     - val loss = 78.1520 (min: 77.5991)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = False\n",
    " - **Modified World Models + BatchNorm (not on last encoding layer)**\n",
    "     - val loss = 76.4446 (min: 76.0836)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = True\n",
    " - **Modified World Models + BatchNorm (including last encoding layer)**\n",
    "     - val loss = 76.3589 (min: 76.1684)\n",
    "     - zdims = 20\n",
    "     - beta = 1\n",
    "     - batch size = 64\n",
    "     - batchnorm = True\n",
    " - **Modified World Models + BatchNorm (not on last encoding layer) w/ beta**\n",
    "     - val loss = 85.2611 (min: 85.2245)\n",
    "     - zdims = 20\n",
    "     - beta = 5\n",
    "     - batch size = 64\n",
    "     - batchnorm = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Although the results are good, it is unclear from the validation error alone whether or not the latent vectors are being disentangled in a way that features can be extracted. Next I will use the latent vectors at various beta levels to test the classification effectiveness/accuracy. Additionally, while the VAE generalizes well, it is not effective in situations when the reconstructed image's quality (clarity / bluriness) cannot be sacrificed -- for this reason, I will also test the performance of the MMD-VAE (InfoVAE).\n",
    "\n",
    "https://arxiv.org/pdf/1706.02262.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
